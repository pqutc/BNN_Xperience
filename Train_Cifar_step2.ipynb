{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29297ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.cuda import amp\n",
    "from scipy.io import savemat\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from Network.ReActNet_18_Qaw import *\n",
    "from Network.ReActNet_A_Qaw import *\n",
    "from Network.utils import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7ba8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Begin_epoch = 0\n",
    "Max_epoch = 15#256\n",
    "Learning_rate = 1e-3\n",
    "Weight_decay = 0\n",
    "Momentum = 0.9\n",
    "Top_k = 5\n",
    "AMP = False\n",
    "\n",
    "Dataset_path = './tests/data/CIFAR10/'\n",
    "Batch_size = 256\n",
    "Workers = 8\n",
    "Targetnum = 10\n",
    "\n",
    "Test_every_iteration = None\n",
    "Name_suffix = '_step2'\n",
    "Savemodel_path = './savemodels/'\n",
    "Record_path = './recorddata/'\n",
    "if not os.path.exists(Savemodel_path):\n",
    "    os.mkdir(Savemodel_path)\n",
    "if not os.path.exists(Record_path):\n",
    "    os.mkdir(Record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a24b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed_ = 2023\n",
    "torch.manual_seed(_seed_)\n",
    "np.random.seed(_seed_)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d1faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Pad(4, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    " \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "Train_data = datasets.CIFAR10(root=Dataset_path, train=True, download=True, transform=transform_train)\n",
    "Test_data = datasets.CIFAR10(root=Dataset_path, train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=Train_data,\n",
    "    batch_size=Batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=Workers, \n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=Test_data,\n",
    "    batch_size=Batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=Workers, \n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fb2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = ResNet18(num_classes=Targetnum, imagenet=False)\n",
    "#net = Reactnet(num_classes=Targetnum, imagenet=False)\n",
    "net = ReactnetNoLoop(num_classes=Targetnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af79190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = nn.DataParallel(net).cuda()\n",
    "max_test_acc = 0.\n",
    "\"\"\"if Begin_epoch!=0:\n",
    "    net.load_state_dict(torch.load(Savemodel_path + f'epoch{Begin_epoch-1}{Name_suffix}.h5'))\n",
    "    max_test_acc = np.load(Savemodel_path + f'max_acc{Name_suffix}.npy')\n",
    "    max_test_acc = max_test_acc.item()\n",
    "else:\n",
    "    net.load_state_dict(torch.load(Savemodel_path + f'max_acc_step1.h5'))\"\"\"\n",
    "\n",
    "\n",
    "scaler = amp.GradScaler() if AMP else None\n",
    "Test_top1 = []\n",
    "Test_topk = []\n",
    "Test_lossall = []\n",
    "Epoch_list = []\n",
    "Iteration_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ef575f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"optimizer = torch.optim.Adam([\\n    {'params' : net.parameters(), 'weight_decay' : Weight_decay, 'initial_lr': Learning_rate}],\\n    lr = Learning_rate)\\nlr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step : (1.0-step/Max_epoch), last_epoch=Begin_epoch-1)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_train = nn.CrossEntropyLoss()#DistributionLoss()\n",
    "criterion_test = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "\"\"\"optimizer = torch.optim.Adam([\n",
    "    {'params' : net.parameters(), 'weight_decay' : Weight_decay, 'initial_lr': Learning_rate}],\n",
    "    lr = Learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step : (1.0-step/Max_epoch), last_epoch=Begin_epoch-1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1151c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, max_test_acc, data_loader=test_data_loader, criterion=criterion_test, epoch=None, iteration=None, record=True):\n",
    "    net.eval()\n",
    "    test_samples = 0\n",
    "    test_loss = 0\n",
    "    test_acc_top1 = 0\n",
    "    test_acc_topk = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(data_loader):\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            label_onehot = F.one_hot(label, Targetnum).float()\n",
    "            \n",
    "            out_fr = net(img)\n",
    "            loss = criterion(out_fr, label)\n",
    "                \n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "\n",
    "            test_acc_top1 += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            _, pred = out_fr.topk(Top_k, 1, True, True)\n",
    "            test_acc_topk += torch.eq(pred, label.view(-1,1)).float().sum().item()\n",
    "    \n",
    "    test_loss /= test_samples\n",
    "    test_acc_top1 /= test_samples\n",
    "    test_acc_topk /= test_samples\n",
    "\n",
    "    if test_acc_top1 >= max_test_acc:\n",
    "        max_test_acc = test_acc_top1\n",
    "        torch.save(net.state_dict(), Savemodel_path + f'max_acc{Name_suffix}.h5')\n",
    "        np.save(Savemodel_path + f'max_acc{Name_suffix}.npy', np.array(max_test_acc))\n",
    "\n",
    "    if record:\n",
    "        assert epoch is not None, \"epoch is None!\"\n",
    "        assert iteration is not None, \"iteration is None!\"\n",
    "        \n",
    "        Epoch_list.append(epoch+1)\n",
    "        Iteration_list.append(iteration+1)\n",
    "        Test_top1.append(test_acc_top1)\n",
    "        Test_topk.append(test_acc_topk)\n",
    "        Test_lossall.append(test_loss)\n",
    "\n",
    "        record_data = np.array([Epoch_list, Iteration_list, Test_top1, Test_topk, Test_lossall]).T\n",
    "        mdic = {f'Record_data':record_data, f'Record_meaning':['Epoch_list', 'Iteration_list', 'Test_top1', f'Test_top{Top_k}', 'Test_loss']}\n",
    "\n",
    "        savemat(Record_path + f'Test_{Begin_epoch}_{epoch}{Name_suffix}.mat',mdic)\n",
    "        if os.path.exists(Record_path + f'Test_{Begin_epoch}_{epoch-1}{Name_suffix}.mat'):\n",
    "            os.remove(Record_path + f'Test_{Begin_epoch}_{epoch-1}{Name_suffix}.mat')\n",
    "\n",
    "    return test_loss, test_acc_top1, test_acc_topk, max_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f67ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, max_test_acc, epoch, data_loader=train_data_loader, optimizer=optimizer, criterion=criterion_test, scaler=scaler, record=True):\n",
    "    train_samples = 0\n",
    "    train_loss = 0\n",
    "    train_acc_top1 = 0\n",
    "    train_acc_topk = 0\n",
    "    \n",
    "    for i, (img, label) in enumerate(tqdm(data_loader)):\n",
    "        net.train()\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "        label_onehot = F.one_hot(label, Targetnum).float()\n",
    "        \n",
    "        if AMP:\n",
    "            with amp.autocast():\n",
    "                out_fr = net(img)\n",
    "                loss = criterion(out_fr, label)\n",
    "        else:\n",
    "            out_fr = net(img)\n",
    "            loss = criterion(out_fr, label)\n",
    "            \n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "\n",
    "        train_acc_top1 += (out_fr.argmax(1) == label).float().sum().item()\n",
    "        _, pred = out_fr.topk(Top_k, 1, True, True)\n",
    "        train_acc_topk += torch.eq(pred, label.view(-1,1)).float().sum().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if AMP:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "            parameters_list = []\n",
    "            for name, p in net.named_parameters():\n",
    "                if not 'fc' in name:\n",
    "                    parameters_list.append(p)\n",
    "            adaptive_clip_grad(parameters_list, clip_factor=0.001)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "        if Test_every_iteration is not None:\n",
    "            if (i+1) % Test_every_iteration == 0:\n",
    "                test_loss, test_acc_top1, test_acc_topk, max_test_acc = test_model(net, max_test_acc, epoch=epoch, iteration=i, record=record)\n",
    "                print(f'Test_loss: {test_loss:.4f}, Test_acc_top1: {test_acc_top1:.4f}, Test_acc_top{Top_k}: {test_acc_topk:.4f}, Max_test_acc: {max_test_acc:.4f}')\n",
    "    \n",
    "    train_loss /= train_samples\n",
    "    train_acc_top1 /= train_samples\n",
    "    train_acc_topk /= train_samples\n",
    "\n",
    "    test_loss, test_acc_top1, test_acc_topk, max_test_acc = test_model(net, max_test_acc, epoch=epoch, iteration=i, record=record)\n",
    "        \n",
    "    return train_loss, train_acc_top1, train_acc_topk, test_loss, test_acc_top1, test_acc_topk, max_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e8d52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:15<00:00, 12.82it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 35.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, train_acc_top1=0.3698, train_acc_top5=0.8781, train_loss=1.7076, test_top1=0.4426, test_top5=0.9092, test_loss=1.5568, max_test_acc=0.4426, total_time=16.3500, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:15<00:00, 12.97it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 37.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, train_acc_top1=0.4435, train_acc_top5=0.9132, train_loss=1.5278, test_top1=0.4760, test_top5=0.9272, test_loss=1.4563, max_test_acc=0.4760, total_time=16.1367, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:14<00:00, 13.07it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 37.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2, train_acc_top1=0.4780, train_acc_top5=0.9278, train_loss=1.4444, test_top1=0.4817, test_top5=0.9327, test_loss=1.4087, max_test_acc=0.4817, total_time=15.9985, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:15<00:00, 12.99it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 37.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3, train_acc_top1=0.5002, train_acc_top5=0.9351, train_loss=1.3803, test_top1=0.5277, test_top5=0.9422, test_loss=1.3224, max_test_acc=0.5277, total_time=16.0923, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:15<00:00, 12.80it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 36.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4, train_acc_top1=0.5175, train_acc_top5=0.9394, train_loss=1.3400, test_top1=0.5557, test_top5=0.9479, test_loss=1.2694, max_test_acc=0.5557, total_time=16.3331, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:15<00:00, 12.90it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 36.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5, train_acc_top1=0.5311, train_acc_top5=0.9440, train_loss=1.3105, test_top1=0.5228, test_top5=0.9425, test_loss=1.3206, max_test_acc=0.5557, total_time=16.2273, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:15<00:00, 12.83it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 37.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6, train_acc_top1=0.5433, train_acc_top5=0.9459, train_loss=1.2828, test_top1=0.5560, test_top5=0.9513, test_loss=1.2513, max_test_acc=0.5560, total_time=16.2768, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:14<00:00, 13.57it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 40.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7, train_acc_top1=0.5504, train_acc_top5=0.9488, train_loss=1.2563, test_top1=0.5603, test_top5=0.9503, test_loss=1.2256, max_test_acc=0.5603, total_time=15.3879, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:13<00:00, 14.06it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 40.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8, train_acc_top1=0.5641, train_acc_top5=0.9507, train_loss=1.2248, test_top1=0.5708, test_top5=0.9563, test_loss=1.2026, max_test_acc=0.5708, total_time=14.8686, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:13<00:00, 14.25it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 40.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9, train_acc_top1=0.5712, train_acc_top5=0.9523, train_loss=1.2081, test_top1=0.5756, test_top5=0.9571, test_loss=1.2018, max_test_acc=0.5756, total_time=14.6809, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:13<00:00, 14.38it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 40.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10, train_acc_top1=0.5778, train_acc_top5=0.9537, train_loss=1.1923, test_top1=0.5968, test_top5=0.9593, test_loss=1.1443, max_test_acc=0.5968, total_time=14.5623, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:13<00:00, 14.14it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 38.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=11, train_acc_top1=0.5817, train_acc_top5=0.9548, train_loss=1.1797, test_top1=0.5847, test_top5=0.9584, test_loss=1.1607, max_test_acc=0.5968, total_time=14.8368, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:13<00:00, 13.96it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 40.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=12, train_acc_top1=0.5887, train_acc_top5=0.9559, train_loss=1.1630, test_top1=0.5921, test_top5=0.9606, test_loss=1.1381, max_test_acc=0.5968, total_time=14.9647, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:13<00:00, 14.03it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 40.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=13, train_acc_top1=0.5928, train_acc_top5=0.9582, train_loss=1.1476, test_top1=0.6024, test_top5=0.9606, test_loss=1.1223, max_test_acc=0.6024, total_time=14.8869, LR=0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:15<00:00, 12.86it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 36.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=14, train_acc_top1=0.5949, train_acc_top5=0.9579, train_loss=1.1433, test_top1=0.5753, test_top5=0.9579, test_loss=1.2022, max_test_acc=0.6024, total_time=16.2629, LR=0.00100000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(Begin_epoch, Max_epoch):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc_top1, train_acc_topk, test_loss, test_acc_top1, test_acc_topk, max_test_acc = train_model(net, max_test_acc, epoch)\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "        \n",
    "    #lr_scheduler.step()\n",
    "\n",
    "    print(f'''epoch={epoch}, train_acc_top1={train_acc_top1:.4f}, train_acc_top{Top_k}={train_acc_topk:.4f}, train_loss={train_loss:.4f}, test_top1={test_acc_top1:.4f}, test_top{Top_k}={test_acc_topk:.4f}, test_loss={test_loss:.4f}, max_test_acc={max_test_acc:.4f}, total_time={(time.time() - start_time):.4f}, LR={lr:.8f}''')\n",
    "    \n",
    "    torch.save(net.state_dict(), Savemodel_path + f'epoch{epoch}{Name_suffix}.h5')\n",
    "    if os.path.exists(Savemodel_path + f'epoch{epoch-1}{Name_suffix}.h5'):\n",
    "        os.remove(Savemodel_path + f'epoch{epoch-1}{Name_suffix}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e69091",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(Savemodel_path + f'max_acc{Name_suffix}.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4998ffd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 22.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_Matrix = tensor([[462., 147.,  40.,  20.,   4.,  15.,  25.,   3., 234.,  50.],\n",
      "        [ 12., 870.,   0.,   5.,   0.,   1.,  10.,   3.,  20.,  79.],\n",
      "        [ 94.,  56., 312., 105.,  51.,  85., 223.,  16.,  47.,  11.],\n",
      "        [ 13.,  41.,  48., 456.,  15., 182., 157.,  14.,  42.,  32.],\n",
      "        [ 26.,  26.,  87.,  75., 322.,  57., 282.,  60.,  47.,  18.],\n",
      "        [  6.,  24.,  32., 231.,  28., 541.,  79.,  21.,  19.,  19.],\n",
      "        [  8.,  26.,  22.,  61.,   3.,  22., 835.,   1.,  16.,   6.],\n",
      "        [ 31.,  50.,  27.,  71.,  70., 123.,  53., 492.,  18.,  65.],\n",
      "        [ 32., 112.,   6.,  13.,   2.,   3.,   9.,   3., 777.,  43.],\n",
      "        [ 20., 219.,   3.,  10.,   1.,   2.,  13.,   2.,  44., 686.]])\n",
      "acc = 0.5752999782562256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Confusion_Matrix = torch.zeros((Targetnum, Targetnum))\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for img, label in tqdm(test_data_loader):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "        out_fr = net(img)\n",
    "        guess = out_fr.argmax(1)\n",
    "        for j in range(len(label)):\n",
    "            Confusion_Matrix[label[j],guess[j]] += 1\n",
    "acc = Confusion_Matrix.diag()\n",
    "acc = acc.sum()/Confusion_Matrix.sum()\n",
    "print(f'Confusion_Matrix = {Confusion_Matrix}')\n",
    "print(f'acc = {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
